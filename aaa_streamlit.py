# https://biz.typecast.ai/org/overview  <= íƒ€ì…ìºìŠ¤íŠ¸ API 

import cv2
import mediapipe as mp
import numpy as np
import torch
import requests
import json
import time
import os
import io

from functions.model import TimeSeriesTransformer
from PIL import ImageFont, ImageDraw, Image
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

import sounddevice as sd
from scipy.io.wavfile import read
# from playsound import playsound  # TTS ìŒì„± íŒŒì¼ ì¬ìƒì„ ìœ„í•´ ì¶”ê°€

import streamlit as st

from langchain.chat_models import ChatOpenAI

import warnings
warnings.filterwarnings('ignore')


def convert_text_to_speech(text, api_key, character_id):
    """
    Convert text to speech using Typecast API í…ìŠ¤íŠ¸ë¥¼ Typecast APIë¥¼ ì‚¬ìš©í•˜ì—¬ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜.
    
    Args:
    text (str): ìŒì„±ìœ¼ë¡œ ë³€í™˜í•  í…ìŠ¤íŠ¸
    api_key (str): Typecast APIì˜ ì¸ì¦ í‚¤
    character_id (str): Typecastì—ì„œ ì‚¬ìš©í•  ìºë¦­í„°ì˜ ID
    
    ê¸°ëŠ¥:
    ì´ í•¨ìˆ˜ëŠ” Typecast APIë¥¼ í˜¸ì¶œí•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ê³ , ë³€í™˜ëœ ìŒì„±ì„ ì¬ìƒí•©ë‹ˆë‹¤.
    """
    
    # API ìš”ì²­ í—¤ë” ì„¤ì • (Authorization: Bearer í† í° ë°©ì‹ìœ¼ë¡œ ì¸ì¦)
    HEADERS = {'Authorization': f'Bearer {api_key}'}

    try:
        # 1. í…ìŠ¤íŠ¸ ìŒì„± ë³€í™˜ ìš”ì²­ (POST ìš”ì²­)yt
        # 'text'ì™€ 'character_id'ë¥¼ í¬í•¨í•˜ì—¬ APIì— ìŒì„± ë³€í™˜ ìš”ì²­ì„ ë³´ëƒ…ë‹ˆë‹¤.
        r = requests.post('https://typecast.ai/api/speak', headers=HEADERS, json={
            'text': text,   # ìŒì„±ìœ¼ë¡œ ë³€í™˜í•  í…ìŠ¤íŠ¸
            'lang': 'auto', # ìë™ ì–¸ì–´ ê°ì§€
            'actor_id': character_id, # ì‚¬ìš©í•  ìºë¦­í„°ì˜ ID
            'xapi_hd': True, # ì¼ë¶€ íŒŒë¼ë¯¸í„° ì„¤ì • (í™”ì§ˆ ë“±ì„ í¬í•¨í•œ ê³ ê¸‰ ì˜µì…˜)
            'model_version': 'latest' # ìµœì‹  ëª¨ë¸ ë²„ì „ ì‚¬ìš©
        })

        # ìš”ì²­ ì„±ê³µ ì‹œ ì‘ë‹µì—ì„œ ìŒì„± íŒŒì¼ URLì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        speak_url = r.json()['result']['speak_v2_url']

         # 2. ìŒì„± ë³€í™˜ ì‘ì—…ì´ ì™„ë£Œë  ë•Œê¹Œì§€ í´ë§(polling)
        for _ in range(60): # ìµœëŒ€ 60ë²ˆ ë°˜ë³µ (1ë¶„ ë™ì•ˆ ê¸°ë‹¤ë¦¼)
            # ë³€í™˜ëœ ìŒì„±ì˜ ìƒíƒœë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ GET ìš”ì²­
            r = requests.get(speak_url, headers=HEADERS)

            # API ì‘ë‹µì—ì„œ ê²°ê³¼ë¥¼ ì¶”ì¶œ
            ret = r.json()['result']

            # ìŒì„± ë³€í™˜ì´ ì™„ë£Œëœ ê²½ìš°
            if ret['status'] == 'done':
                # 3. ìŒì„± ë‹¤ìš´ë¡œë“œ ë§í¬ë¥¼ í†µí•´ ìŒì„± ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
                audio_data = requests.get(ret['audio_download_url']).content  # ë°”ì´ë„ˆë¦¬ ë°ì´í„°ë¡œ ë‹¤ìš´ë¡œë“œ
                
                # ë‹¤ìš´ë¡œë“œí•œ ìŒì„± ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ ë‚´ì—ì„œ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ë³€í™˜
                audio_stream = io.BytesIO(audio_data)  

                # 4. ìŒì„± íŒŒì¼ì„ ì½ì–´ ë“¤ì—¬ ìƒ˜í”Œë ˆì´íŠ¸ì™€ ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ì–»ìŠµë‹ˆë‹¤.
                sample_rate, audio = read(audio_stream)  

                # 5. ìŒì„± ì¬ìƒ (sounddeviceë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ ì¬ìƒ)
                sd.play(audio, samplerate=sample_rate)  # ì˜¤ë””ì˜¤ ì¬ìƒ
                sd.wait()  # ì˜¤ë””ì˜¤ê°€ ëë‚  ë•Œê¹Œì§€ ëŒ€ê¸°

                break # ìŒì„± ë³€í™˜ì´ ì™„ë£Œë˜ë©´ ë£¨í”„ ì¢…ë£Œ

    except Exception as e:
        # ì˜ˆì™¸ ë°œìƒ ì‹œ ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶œë ¥
        print(f"TTS conversion error: {e}")


# Typecast API Configuration
TYPECAST_API_KEY = ""
TYPECAST_CHARACTER_ID = "622964d6255364be41659078"
# "66d000ee0742c43c93a0ada1" ë‚¨ì ëª©ì†Œë¦¬ : ë„í˜„
# "65bb3a1976b69213594357fc" ì—¬ì ëª©ì†Œë¦¬ : ì§„ì„œ

def speak(text):
    # Authorization í—¤ë”ë¥¼ ì‚¬ìš©í•˜ì—¬ Typecast API ì¸ì¦ì„ ìœ„í•œ API í‚¤ ì„¤ì •
    headers = {'Authorization': f'Bearer {TYPECAST_API_KEY}'}
    
    # POST ìš”ì²­ì„ ë³´ë‚´ í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ìš”ì²­ì„ Typecast APIë¡œ ì „ì†¡
    r = requests.post('https://typecast.ai/api/speak', headers=headers, json={
        'text': text,   # ë³€í™˜í•  í…ìŠ¤íŠ¸ë¥¼ ì „ë‹¬
        'lang': 'auto', # ìë™ ì–¸ì–´ ê°ì§€
        'actor_id': TYPECAST_CHARACTER_ID,   # ì‚¬ìš©í•  ìŒì„± ë°°ìš°(ìºë¦­í„°) ID ì„¤ì • 
        'xapi_hd': True,    # ê³ í•´ìƒë„ ì˜¤ë””ì˜¤ ìš”ì²­
        'model_version': 'latest'  # ìµœì‹  ëª¨ë¸ ì‚¬ìš©
    })
    
    # API ì‘ë‹µì—ì„œ ìŒì„± í•©ì„± URL ì¶”ì¶œ
    speak_url = r.json()['result']['speak_v2_url']

     # ìŒì„± í•©ì„± ê²°ê³¼ê°€ ì™„ë£Œë  ë•Œê¹Œì§€ 60ë²ˆê¹Œì§€ í™•ì¸í•˜ëŠ” ë°˜ë³µë¬¸
    for _ in range(60):
        # ìŒì„± í•©ì„± ìƒíƒœë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ GET ìš”ì²­
        r = requests.get(speak_url, headers=headers)
        ret = r.json()['result'] # ì‘ë‹µì—ì„œ ìŒì„± í•©ì„± ìƒíƒœ ì •ë³´ ì¶”ì¶œ

        # ìŒì„± í•©ì„± ìƒíƒœê°€ ì™„ë£Œë˜ì—ˆìœ¼ë©´
        if ret['status'] == 'done':
            # ìŒì„± íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ê¸° ìœ„í•œ URLì—ì„œ ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ë°›ì•„ì˜´
            audio_data = requests.get(ret['audio_download_url']).content
            # ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ ìƒì—ì„œ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ë³€í™˜
            audio_stream = io.BytesIO(audio_data)  

            # ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ì—ì„œ ìƒ˜í”Œë§ ì£¼íŒŒìˆ˜ì™€ ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ì½ì–´ì˜´
            sample_rate, audio = read(audio_stream)  

            # ì˜¤ë””ì˜¤ë¥¼ ì¬ìƒ
            sd.play(audio, samplerate=sample_rate)  
            
            # ì˜¤ë””ì˜¤ê°€ ëë‚  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼
            sd.wait()  

            # ìŒì„± ì¬ìƒ í›„ ë£¨í”„ ì¢…ë£Œ
            break

# Model and API Initialization
HOME_PATH = 'C:/ai5/Bon_project/main_project/custom-final-custom'
last_added_time = 0
openai_api_key = ''

# LLM ì„¤ì •
llm = ChatOpenAI(
    model_name="gpt-3.5-turbo",
    temperature=0.1,
    api_key=openai_api_key,
)

prompt = PromptTemplate(
    input_variables=["filtered_words"],
    template=(
        "ë¦¬ìŠ¤íŠ¸ì— í¬í•¨ë˜ì§€ ì•Šì€ ë‹¨ì–´ë¥¼ ìƒˆë¡­ê²Œ ìƒì„±í•˜ê±°ë‚˜ ë¬¸ì¥ì„ ìƒˆë¡­ê²Œ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”."
        "ì•„ë˜ì˜ ë‹¨ì–´ë“¤ë§Œ ì‚¬ìš©í•´ì„œ ë¬¸ì–´ì²´ ë¬¸ì¥ì„ ì‘ì„±í•˜ì„¸ìš”. "
        "ë°˜ë“œì‹œ ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” ë‹¨ì–´ë§Œ ì‚¬ìš©í•˜ê³ , í•„ìš” ì—†ëŠ” ë‹¨ì–´ëŠ” ì‚­ì œí•˜ì„¸ìš”. "
        "ì¶œë ¥ì€ ë‹¤ìŒ ì¡°ê±´ì„ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤:\n"
        "1. ë¬¸ì¥ì€ í•œ ë¬¸ì¥ë§Œ ì‘ì„±í•˜ì„¸ìš”.\n"
        "2. ì¶œë ¥ ë¬¸ì¥ì€ ë§ˆì¹¨í‘œ(.)ë¡œ ëë‚˜ì•¼ í•©ë‹ˆë‹¤.\n"
        "3. êµ¬ì–´ì²´ë‚˜ ë¶€ì ì ˆí•œ í‘œí˜„ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.\n"
        "4. ë‹¨ì–´ì˜ ë¬¸ë§¥ì  ì˜ë¯¸ë¥¼ ê³ ë ¤í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ì‚¬ìš©í•˜ì„¸ìš”.\n"
        "5. ë¦¬ìŠ¤íŠ¸ ë‚´ì˜ ë‹¨ì–´ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ì„ ì¶œë ¥í•  ìˆ˜ ì—†ë‹¤ë©´, '-'ë¡œ ì¶œë ¥í•˜ì„¸ìš”\n"
        "ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸: {filtered_words}\n"
        "ì¶œë ¥ ì˜ˆì‹œ:\n"
        "- ì…ë ¥: ['ì•ˆë…•', 'ë§Œë‚˜ë‹¤', 'ë°˜ê°‘ë‹¤']\n"
        "- ì¶œë ¥: 'ì•ˆë…• ë§Œë‚˜ì„œ, ë°˜ê°€ì›Œ'\n"
        "- ì…ë ¥: ['ë°˜ê°‘ë‹¤', 'ë‚˜', 'ì†Œê°œ']\n"
        "- ì¶œë ¥: 'ë‚´ ì†Œê°œë¥¼ í• ê»˜!'\n"
        "- ì…ë ¥: ['ë‚˜', 'ì´ë¦„', 'ì§€í˜œ']\n"
        "- ì¶œë ¥: 'ë‚´ ì´ë¦„ì€ ì§€í˜œì•¼.'\n"
        "- ì…ë ¥: ['ë‚˜', 'íƒœìš´', 'ì´ë¦„', 'íƒœìš´']\n"
        "- ì¶œë ¥: 'ë‚´ ì´ë¦„ì€ íƒœìš´ì´ì•¼.'\n"
        "- ì…ë ¥: ['ë‚˜', 'ì´ë¦„', 'ê¸°í˜„']\n"
        "- ì¶œë ¥: 'ë‚´ ì´ë¦„ì€ ê¸°í˜„ì´ì•¼.  '\n"
        "- ì…ë ¥: ['ë‚˜', 'ì´ˆë°¥', 'ì¢‹ì•„í•´']\n"
        "- ì¶œë ¥: 'ë‚˜ëŠ” ì´ˆë°¥ì„ ì¢‹ì•„í•´.'\n"
        "- ì…ë ¥: ['ì–´ì œ', 'íŒŒìŠ¤íƒ€', 'ë¨¹ë‹¤']\n"
        "- ì¶œë ¥: 'ì–´ì œ íŒŒìŠ¤íƒ€ë¥¼ ë¨¹ì—ˆì–´.'\n"
        "- ì…ë ¥: ['ë‚˜', 'ì±…', 'ì½ë‹¤', 'ì¢‹ì•„í•˜ë‹¤']\n"
        "- ì¶œë ¥: 'ë‚˜ëŠ” ì±…ì„ ì½ëŠ” ê±¸ ì¢‹ì•„í•´.'\n"
        "- ì…ë ¥: ['ë‚˜', 'ì±…', 'ì½ë‹¤', 'ì¢‹ì•„í•˜ë‹¤']\n"
        "- ì¶œë ¥: 'ë‚˜ëŠ” ì±…ì„ ì½ëŠ” ê±¸ ì¢‹ì•„í•´.'\n"
        "- ì…ë ¥: ['ë‚˜', 'ìš”ë¦¬', 'ì¢‹ì•„í•´']\n"
        "- ì¶œë ¥: 'ë‚˜ëŠ” ìš”ë¦¬ë„ ì¢‹ì•„í•´.'\n"
        "- ì…ë ¥: ['ê³ ë§ˆì›Œ']\n"
        "- ì¶œë ¥: 'ê³ ë§ˆì›Œ!'\n"
        "- ì…ë ¥: ['ë‹¤ìŒ', 'ë‹¤ì‹œ', 'ë§Œë‚˜ë‹¤']\n"
        "- ì¶œë ¥: 'ë‹¤ìŒì— ë‹¤ì‹œ ë§Œë‚˜!'\n"
        "ë¦¬ìŠ¤íŠ¸ì— í¬í•¨ë˜ì§€ ì•Šì€ ë‹¨ì–´ë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ ë¬¸ì¥ì„ ì´ˆì›”í•´ì„œ ë§í•˜ì§€ ë§ˆì„¸ìš”."
    )
)

chain = LLMChain(llm=llm, prompt=prompt)

# Model ì„¤ì • ë° ë¶ˆëŸ¬ì˜¤ê¸°
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
num_classes = 37
model = TimeSeriesTransformer(224, 64, 8, 1, 37, 30, 1)
# model_weights_path = f'{HOME_PATH}/models/model_final_custom2.pt'
model_weights_path = 'C:/Users/kim/Desktop/ëŒ€ë‚˜ë¬´íŒ€/custom-final/models/model_final_custom4.pt'
model = torch.load(model_weights_path, map_location=device)

# 0ê³µë°±, 1ì•ˆë…•, 2ë§Œë‚˜ë‹¤, 3ë°˜ê°‘ë‹¤, 4ë‚˜, 5ì…ë‹ˆë‹¤, 6ì´ë¦„, 7ì§€í˜œ, 8ê¸°í˜„, 9íƒœìš´, 
# 10ì¢‹ì•„í•´, 11ì´ˆë°¥, 12íŒŒìŠ¤íƒ€, 13ì±…, 14ìš”ë¦¬, 15ê°ì‚¬í•©ë‹ˆë‹¤, 16ë‹¤ìŒ, 17ë‹¤ì‹œ, 18ì†Œê°œ, 19ìš”ì¦˜
# 20ìˆ˜í™”, 21ë°°ìš°ë‹¤, 22ì»´í“¨í„°, 23ì—´ì‹¬íˆ, 24ë…¸ë ¥, 25ì¤‘ì´ë‹¤, 26ë¬´ì–¼, 27ìƒê°, 28ì–´ì œ, 29ë¨¹ë‹¤, 
# 30ë§›ìˆë‹¤, 31ì´ìœ , 32ì½ë‹¤, 33ë“£ë‹¤, 34ë„ˆ, 35ì¹˜í‚¨, 36ëŒ€í•™ì›

gesture = {0: "", 1: "ì•ˆë…•", 2: "ë§Œë‚˜ë‹¤", 3: "ë°˜ê°‘ë‹¤", 4: "ë‚˜", 5: "ì…ë‹ˆë‹¤", 6: "ì´ë¦„", 7: "ì§€í˜œ", 8: "ê¸°í˜„", 9: "íƒœìš´", 
           10: "ì¢‹ì•„í•´", 11 : "ì´ˆë°¥", 12 : "íŒŒìŠ¤íƒ€", 13 : "ì±…", 14: "ìš”ë¦¬", 15 : "ê³ ë§ˆì›Œ", 16 : "ë‹¤ìŒ", 17 : "ë‹¤ì‹œ", 18: "ì†Œê°œ", 19: "ìš”ì¦˜",
           20: "ìˆ˜í™”", 21: "ë°°ìš°ë‹¤", 22: "ì»´í“¨í„°", 23: "ì—´ì‹¬íˆ", 24: "ë…¸ë ¥", 25:"ì¤‘ì´ë‹¤", 26: "ë¬´ì–¼", 27: "ìƒê°", 28: "ì–´ì œ", 29: "ë¨¹ë‹¤",
           30: "ë§›ìˆë‹¤", 31: "ì´ìœ ", 32: "ì½ë‹¤", 33: "ë“£ë‹¤", 34: "ë„ˆ", 35 : "ì¹˜í‚¨", 36: "ëŒ€í•™ì›"}

actions = ["", "ì•ˆë…•", "ë§Œë‚˜ë‹¤", "ë°˜ê°‘ë‹¤", "ë‚˜", "ì…ë‹ˆë‹¤", "ì´ë¦„", "ì§€í˜œ", "ê¸°í˜„", "íƒœìš´", 
           "ì¢‹ì•„í•´", "ì´ˆë°¥", "íŒŒìŠ¤íƒ€", "ì±…", "ìš”ë¦¬", "ê³ ë§ˆì›Œ", "ë‹¤ìŒ", "ë‹¤ì‹œ", "ì†Œê°œ", "ìš”ì¦˜",
           "ìˆ˜í™”", "ë°°ìš°ë‹¤", "ì»´í“¨í„°", "ì—´ì‹¬íˆ", "ë…¸ë ¥", "ì¤‘ì´ë‹¤", "ë¬´ì–¼", "ìƒê°", "ì–´ì œ", "ë¨¹ë‹¤",
           "ë§›ìˆë‹¤", "ì´ìœ ", "ì½ë‹¤", "ë“£ë‹¤", "ë„ˆ", "ì¹˜í‚¨", "ëŒ€í•™ì›"]

seq_length = 30


st.set_page_config(page_title="ìˆ˜í™” ë²ˆì—­", page_icon='ğŸ™Œ')
st.markdown("<h1 style='text-align: center;'> ìˆ˜í™” ë²ˆì—­ </h1>", unsafe_allow_html=True)


    

# MediaPipe ì„¤ì • ë° ì´ˆê¸°í™” ì¶”ê°€
mp_holistic = mp.solutions.holistic
mp_drawing = mp.solutions.drawing_utils
holistic = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)

cap = cv2.VideoCapture(0)    # "http://192.168.0.76:8080/video"

cap.set(cv2.CAP_PROP_FPS, 20) 
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

seq = []
action_seq = []
if 'lang' not in st.session_state:
    st.session_state.lang = []

font = ImageFont.truetype("malgunbd.ttf", 40)

frame_placeholder = st.empty()
text_placeholder = st.empty()
text_placeholder2 = st.empty()
state = False

if st.button('í™•ì¸'):
    state = True

def add_word(word):
    st.session_state.lang.append(word)

while cap.isOpened():
    ret, img = cap.read()
    
    if not ret:
        print("ì¹´ë©”ë¼ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        break

    img = cv2.flip(img, 1)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    result = holistic.process(img)
    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)

    mp_drawing.draw_landmarks(img, result.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)
    mp_drawing.draw_landmarks(img, result.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)
    mp_drawing.draw_landmarks(img, result.pose_landmarks, mp_holistic.POSE_CONNECTIONS)

    # cv2.imshow('img', img)

    # key = cv2.waitKey(1)

    if result.right_hand_landmarks is None and result.left_hand_landmarks is None and result.pose_landmarks is None:
        continue

    right_joint_angle = np.array([])

    if result.right_hand_landmarks is not None:
        joint = np.zeros((21, 4))

        for j, lm in enumerate(result.right_hand_landmarks.landmark):
            joint[j] = [lm.x, lm.y, lm.z, lm.visibility]

        v1 = joint[[0, 1, 2, 3, 0, 5, 6, 7, 0, 9, 10, 11, 0, 13, 14, 15, 0, 17, 18, 19], :]
        v2 = joint[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], :]

        v = v2 - v1

        v = v / np.linalg.norm(v, axis=1)[:, np.newaxis]

        angle = np.arccos(
            np.einsum(
                "nt,nt->n",
                v[[0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 14, 16, 17, 18], :],
                v[[1, 2, 3, 5, 6, 7, 9, 10, 11, 13, 14, 15, 17, 18, 19], :],
            )
        )

        angle = np.degrees(angle)
        angle = np.array([angle], dtype=np.float32)
        right_joint_angle = np.concatenate([joint.flatten(), angle.reshape(angle.shape[1],)])

    else:
        right_joint_angle = np.zeros((99, ))

    left_joint_angle = np.array([])

    if result.left_hand_landmarks is not None:
        joint = np.zeros((21, 4))

        for j, lm in enumerate(result.left_hand_landmarks.landmark):
            joint[j] = [lm.x, lm.y, lm.z, lm.visibility]

        v1 = joint[[0, 1, 2, 3, 0, 5, 6, 7, 0, 9, 10, 11, 0, 13, 14, 15, 0, 17, 18, 19], :]
        v2 = joint[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], :]

        v = v2 - v1

        v = v / np.linalg.norm(v, axis=1)[:, np.newaxis]

        angle = np.arccos(
            np.einsum(
                "nt,nt->n",
                v[[0, 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 14, 16, 17, 18], :],
                v[[1, 2, 3, 5, 6, 7, 9, 10, 11, 13, 14, 15, 17, 18, 19], :],
            )
        )

        angle = np.degrees(angle)
        angle = np.array([angle], dtype=np.float32)

        left_joint_angle = np.concatenate([joint.flatten(), angle.reshape(angle.shape[1],)])
    else:
        left_joint_angle = np.zeros((99, ))

    pose_joint_angle = np.array([])

    if result.pose_landmarks is not None:
        joint = np.zeros((6, 4))

        k = 0

        for j, lm in enumerate(result.pose_landmarks.landmark):
            if j in [11, 12, 13, 14, 23, 24]:
                joint[k] = [lm.x, lm.y, lm.z, lm.visibility]

                k += 1

        v1 = joint[[0, 1, 1, 0, 0, 4], :]
        v2 = joint[[1, 3, 5, 2, 4, 5], :]

        v = v2 - v1

        v = v / np.linalg.norm(v, axis=1)[:, np.newaxis]

        angle = np.arccos(
            np.einsum(
                "nt,nt->n",
                v[[0, 1], :],
                v[[2, 3], :],
            )
        )

        angle = np.degrees(angle)
        angle = np.array([angle], dtype=np.float32)

        pose_joint_angle = np.concatenate([joint.flatten(), angle.reshape(angle.shape[1],)])

    else:   
        pose_joint_angle = np.zeros((27, ))

    data = np.concatenate([right_joint_angle, left_joint_angle, pose_joint_angle])
    seq.append(data)

    # ì‹œí€€ìŠ¤ê°€ ì¶©ë¶„íˆ ìŒ“ì˜€ëŠ”ì§€ í™•ì¸ (ê¸¸ì´ê°€ seq_lengthì™€ ë™ì¼í•˜ê±°ë‚˜ ê·¸ ì´ìƒì¸ì§€ í™•ì¸)
    if len(seq) < seq_length:
        continue
    
    # seqì˜ ë§ˆì§€ë§‰ seq_lengthë§Œí¼ ìŠ¬ë¼ì´ì‹±í•˜ì—¬ ëª¨ë¸ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
    input_data = np.array(seq[-seq_length:], dtype=np.float32)  # (seq_length, feature_dim)
    
    # ì—¬ê¸°ì„œ 3ì°¨ì› í…ì„œë¡œ ë³€ê²½í•˜ê¸° ìœ„í•´ batch ì°¨ì›ì„ ì¶”ê°€í•¨
    input_data = np.expand_dims(input_data, axis=0)  # (1, seq_length, feature_dim)
    
    # í…ì„œë¥¼ Torch í…ì„œë¡œ ë³€í™˜í•˜ê³ , ì¥ì¹˜ì— ë§ê²Œ ì˜¬ë¦¼
    input_data = torch.FloatTensor(input_data).to(device)  # ìµœì¢… í˜•íƒœ: (batch_size, seq_length, feature_dim)

    # ëª¨ë¸ ì˜ˆì¸¡ 
    y_pred = model(input_data) # (batch_size, num_classes)
    
    # ìµœëŒ€ í™•ë¥ ê°’ê³¼ ê·¸ ì¸ë±ìŠ¤ë¥¼ ê°€ì ¸ì˜´
    conf, idx = torch.max(y_pred.data, dim=1)
    
    if conf < 0.25:
        continue
    
    # ì˜ˆì¸¡ëœ ë™ì‘ì„ ê°€ì ¸ì˜´
    action = actions[idx.item()]
    action_seq.append(action)

    if len(action_seq) < 10:
        continue
    
    this_action = ''

    # ì´ ì¡°ê±´ë¬¸ì€ action_seq ë¦¬ìŠ¤íŠ¸ì˜ ë§ˆì§€ë§‰ 5ê°œ ìš”ì†Œê°€ ëª¨ë‘ ê°™ì€ì§€ í™•ì¸
    if action_seq[-1] == action_seq[-2] == action_seq[-3] == action_seq[-4] == action_seq[-5]:
         this_action = action
    
    # ë§ˆì§€ë§‰ 5ê°œì˜ ì•¡ì…˜ì´ ë™ì¼í•˜ë©´ ì´ë¥¼ this_actionìœ¼ë¡œ ì‚¬ìš©
    if action_seq[-5:] == [action] * 5:
        if time.time() - last_added_time > 2:
             if action and (not st.session_state.lang or st.session_state.lang[-1] != action):  # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì¶”ê°€
                # st.session_state.lang.append(action)
                add_word(action)
                last_added_time = time.time()  # ë§ˆì§€ë§‰ ì¶”ê°€ ì‹œê°„ì„ ê°±ì‹ í•˜ì—¬ ì¤‘ë³µ ì¶”ê°€ ë°©ì§€
                print(f"Added to st.session_state.lang: {action}")
                text_placeholder2.info(f'ì¶”ê°€ëœ ì „ì²´ ë‹¨ì–´ : {st.session_state.lang}')
                print(st.session_state.lang)
    
    # ì´ë¯¸ì§€ì— í˜„ì¬ ì•¡ì…˜ í‘œì‹œ
    if action != '':
        pil_img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
        draw = ImageDraw.Draw(pil_img)
        draw.text((10, 30), f'[{action}]', font=font, fill=(255, 0, 0))
        img = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)
    
    frame_placeholder.image(img, channels='BGR')

    # key = cv2.waitKey(1)
    # ì‹œí€€ìŠ¤ê°€ ë„ˆë¬´ ê¸¸ë©´ ì´ˆê¸°í™”
    if len(seq) >= 300:
        seq = []

    if len(action_seq) >= 300:
        action_seq = []

    # cv2.imshow('img', img)
    
    if cv2.waitKey(1) == ord('q'):
        break

    # # ë°±ìŠ¤í˜ì´ìŠ¤ í‚¤ë¡œ ë§ˆì§€ë§‰ ì•¡ì…˜ ì œê±°
    # if key == 8:
    #     if st.session_state.lang:
    #         st.session_state.lang.pop()
    #         print("ì‚­ì œ", st.session_state.lang)
        
    # print('lang í™•ì¸ìš© :', st.session_state.lang)

     # ìŠ¤í˜ì´ìŠ¤ í‚¤ë¡œ í˜„ì¬ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° ë¬¸ì¥ ìƒì„± ë° ìŒì„± ì¶œë ¥
    if state:
        # state = False
        # cap.release()

        print('st.session_state.lang : ', st.session_state.lang)
        # filtered_words = [word for word in st.session_state.lang if word.strip()]
        filtered_words = st.session_state.lang
        print('filtered_words :', filtered_words)

        if filtered_words:
            # result_text = chain.run(filtered_words=" ".join(filtered_words))
            result_text = chain.run(filtered_words)
            print("ì¶œë ¥:", result_text)
            
            # 'ì¶œë ¥:' ì´í›„ì˜ ë¬¸ì¥ë§Œ TTSë¡œ ë³€í™˜í•˜ì—¬ ì½ì–´ì¤Œ
            output_text = result_text.split(': ')[-1].strip().strip("'")
            # output_text = result_text.strip().strip("'")
            text_placeholder.info(f'ë¬¸ì¥ : {output_text}')
            convert_text_to_speech(output_text, TYPECAST_API_KEY, TYPECAST_CHARACTER_ID)
            # speak(output_text)
            
            st.session_state.lang = []
            state = False
            st.rerun()
        else:
            print("ìƒì„±í•  ë¬¸ì¥ì´ ì—†ìŠµë‹ˆë‹¤.")
            state = False
            st.session_state.lang = []
            st.rerun()

       
            
    #     if key == ord('q'):
    #         break
    
    # if key == ord(' '):  # ìŠ¤í˜ì´ìŠ¤ë°” ëˆŒë €ì„ ë•Œ ë¬¸ì¥ ìƒì„± ë° TTS ì‹¤í–‰
    #     st.session_state.lang = []

    # if key == 8:  # ë°±ìŠ¤í˜ì´ìŠ¤ í‚¤ë¥¼ ëˆŒë €ì„ ë•Œ ë§ˆì§€ë§‰ ë‹¨ì–´ ì‚­ì œ
    #     if st.session_state.lang:
    #         st.session_state.lang.pop()
    #         print("ì‚­ì œ", st.session_state.lang)

    if result.right_hand_landmarks is None and result.left_hand_landmarks is None and result.pose_landmarks is None:
        continue
