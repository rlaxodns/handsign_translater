import cv2
import mediapipe as mp
import numpy as np
import torch
import requests
import json
import time
import os
import io

import streamlit as st
import sounddevice as sd
from scipy.io.wavfile import read

from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import ChatOpenAI
from PIL import ImageFont, ImageDraw, Image

# ===== ê¸°ì¡´ì— ì‚¬ìš©í•˜ë˜ í•¨ìˆ˜ë“¤ =====

def convert_text_to_speech(text, api_key, character_id):
    HEADERS = {'Authorization': f'Bearer {api_key}'}
    try:
        r = requests.post('https://typecast.ai/api/speak', headers=HEADERS, json={
            'text': text,
            'lang': 'auto',
            'actor_id': character_id,
            'xapi_hd': True,
            'model_version': 'latest'
        })

        speak_url = r.json()['result']['speak_v2_url']

        for _ in range(60): 
            r = requests.get(speak_url, headers=HEADERS)
            ret = r.json()['result']

            if ret['status'] == 'done':
                audio_data = requests.get(ret['audio_download_url']).content
                audio_stream = io.BytesIO(audio_data)
                sample_rate, audio = read(audio_stream)
                sd.play(audio, samplerate=sample_rate)
                sd.wait()
                break
    except Exception as e:
        print(f"TTS conversion error: {e}")

# ===== Typecast API í‚¤ ë° ì„¤ì • =====
TYPECAST_API_KEY = ""

def speak(text):
    headers = {'Authorization': f'Bearer {TYPECAST_API_KEY}'}
    r = requests.post('https://typecast.ai/api/speak', headers=headers, json={
        'text': text,
        'lang': 'auto',
        'actor_id': TYPECAST_CHARACTER_ID,
        'xapi_hd': True,
        'model_version': 'latest'
    })
    speak_url = r.json()['result']['speak_v2_url']

    for _ in range(60):
        r = requests.get(speak_url, headers=headers)
        ret = r.json()['result']
        if ret['status'] == 'done':
            audio_data = requests.get(ret['audio_download_url']).content
            audio_stream = io.BytesIO(audio_data)
            sample_rate, audio = read(audio_stream)
            sd.play(audio, samplerate=sample_rate)
            sd.wait()
            break

# ===== LLM ì„¤ì • =====
openai_api_key = 'sk-...' # ì—¬ê¸°ì— ì‹¤ì œ OpenAI API í‚¤ ì…ë ¥
llm = ChatOpenAI(
    model_name="gpt-3.5-turbo",
    temperature=0.1,
    api_key=openai_api_key,
)

prompt = PromptTemplate(
    input_variables=["filtered_words"],
    template=(
        "ë¦¬ìŠ¤íŠ¸ì— í¬í•¨ë˜ì§€ ì•Šì€ ë‹¨ì–´ë¥¼ ìƒˆë¡­ê²Œ ìƒì„±í•˜ê±°ë‚˜ ë¬¸ì¥ì„ ìƒˆë¡­ê²Œ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”."
        "ì•„ë˜ì˜ ë‹¨ì–´ë“¤ë§Œ ì‚¬ìš©í•´ì„œ ë¬¸ì–´ì²´ ë¬¸ì¥ì„ ì‘ì„±í•˜ì„¸ìš”. "
        "ë°˜ë“œì‹œ ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” ë‹¨ì–´ë§Œ ì‚¬ìš©í•˜ê³ , í•„ìš” ì—†ëŠ” ë‹¨ì–´ëŠ” ì‚­ì œí•˜ì„¸ìš”. "
        "ì¶œë ¥ì€ ë‹¤ìŒ ì¡°ê±´ì„ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤:\n"
        "1. ë¬¸ì¥ì€ í•œ ë¬¸ì¥ë§Œ ì‘ì„±í•˜ì„¸ìš”.\n"
        "2. ì¶œë ¥ ë¬¸ì¥ì€ ë§ˆì¹¨í‘œ(.)ë¡œ ëë‚˜ì•¼ í•©ë‹ˆë‹¤.\n"
        "3. êµ¬ì–´ì²´ë‚˜ ë¶€ì ì ˆí•œ í‘œí˜„ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.\n"
        "4. ë‹¨ì–´ì˜ ë¬¸ë§¥ì  ì˜ë¯¸ë¥¼ ê³ ë ¤í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ì‚¬ìš©í•˜ì„¸ìš”.\n"
        "5. ë¦¬ìŠ¤íŠ¸ ë‚´ì˜ ë‹¨ì–´ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ì„ ì¶œë ¥í•  ìˆ˜ ì—†ë‹¤ë©´, '-'ë¡œ ì¶œë ¥í•˜ì„¸ìš”\n"
        "ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸: {filtered_words}\n"
        "..."
    )
)

chain = LLMChain(llm=llm, prompt=prompt)

# ===== ëª¨ë¸ ë¡œë“œ =====
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
from functions.model import TimeSeriesTransformer  # ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ëŠ” ë¶€ë¶„ì€ ì‚¬ìš©ì í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •
model = TimeSeriesTransformer(224, 64, 8, 1, 37, 30, 1)
model_weights_path = r'C:\Users\kim\Desktop\ëŒ€ë‚˜ë¬´íŒ€\custom-final\models\model_final_custom4.pt'
model = torch.load(model_weights_path, map_location=device)

# ì œìŠ¤ì³ ë¼ë²¨
gesture = {0: "", 1: "ì•ˆë…•", 2: "ë§Œë‚˜ë‹¤", 3: "ë°˜ê°‘ë‹¤", 4: "ë‚˜", 5: "ì…ë‹ˆë‹¤", 6: "ì´ë¦„", 7: "ì§€í˜œ", 8: "ê¸°í˜„", 9: "íƒœìš´", 
           10: "ì¢‹ì•„í•´", 11 : "ì´ˆë°¥", 12 : "íŒŒìŠ¤íƒ€", 13 : "ì±…", 14: "ìš”ë¦¬", 15 : "ê³ ë§ˆì›Œ", 16 : "ë‹¤ìŒ", 17 : "ë‹¤ì‹œ", 18: "ì†Œê°œ", 19: "ìš”ì¦˜",
           20: "ìˆ˜í™”", 21: "ë°°ìš°ë‹¤", 22: "ì»´í“¨í„°", 23: "ì—´ì‹¬íˆ", 24: "ë…¸ë ¥", 25:"ì¤‘ì´ë‹¤", 26: "ë¬´ì–¼", 27: "ìƒê°", 28: "ì–´ì œ", 29: "ë¨¹ë‹¤",
           30: "ë§›ìˆë‹¤", 31: "ì´ìœ ", 32: "ì½ë‹¤", 33: "ë“£ë‹¤", 34: "ë„ˆ", 35 : "ì¹˜í‚¨", 36: "ëŒ€í•™ì›"}

actions = ["", "ì•ˆë…•", "ë§Œë‚˜ë‹¤", "ë°˜ê°‘ë‹¤", "ë‚˜", "ì…ë‹ˆë‹¤", "ì´ë¦„", "ì§€í˜œ", "ê¸°í˜„", "íƒœìš´", 
           "ì¢‹ì•„í•´", "ì´ˆë°¥", "íŒŒìŠ¤íƒ€", "ì±…", "ìš”ë¦¬", "ê³ ë§ˆì›Œ", "ë‹¤ìŒ", "ë‹¤ì‹œ", "ì†Œê°œ", "ìš”ì¦˜",
           "ìˆ˜í™”", "ë°°ìš°ë‹¤", "ì»´í“¨í„°", "ì—´ì‹¬íˆ", "ë…¸ë ¥", "ì¤‘ì´ë‹¤", "ë¬´ì–¼", "ìƒê°", "ì–´ì œ", "ë¨¹ë‹¤",
           "ë§›ìˆë‹¤", "ì´ìœ ", "ì½ë‹¤", "ë“£ë‹¤", "ë„ˆ", "ì¹˜í‚¨", "ëŒ€í•™ì›"]

seq_length = 30

mp_holistic = mp.solutions.holistic
mp_drawing = mp.solutions.drawing_utils


# ===== Streamlit UI ì‹œì‘ =====
st.set_page_config(page_title="ìˆ˜ì–´ ì¸ì‹ Demo", layout="wide")
st.title("ğŸ“¹ ì‹¤ì‹œê°„ ìˆ˜ì–´ ì¸ì‹ Demo")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "running" not in st.session_state:
    st.session_state.running = False
if "seq" not in st.session_state:
    st.session_state.seq = []
if "action_seq" not in st.session_state:
    st.session_state.action_seq = []
if "lang" not in st.session_state:
    st.session_state.lang = []
if "last_added_time" not in st.session_state:
    st.session_state.last_added_time = 0

start_button = st.sidebar.button("ì›¹ìº  ì‹œì‘/ì¤‘ì§€")

if start_button:
    st.session_state.running = not st.session_state.running

st.sidebar.write("ìŠ¤í˜ì´ìŠ¤ë°” ê¸°ëŠ¥ ëŒ€ì‹ , ì•„ë˜ 'ë¬¸ì¥ ìƒì„± ë° ì½ê¸°' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
generate_button = st.sidebar.button("ë¬¸ì¥ ìƒì„± ë° ì½ê¸°")
delete_button = st.sidebar.button("ë§ˆì§€ë§‰ ë‹¨ì–´ ì‚­ì œ")

info_placeholder = st.empty()  # í˜„ì¬ ê°ì§€í•œ ë‹¨ì–´ í‘œì‹œ
frame_placeholder = st.empty() # ë¹„ë””ì˜¤ í”„ë ˆì„ í‘œì‹œ

font = ImageFont.truetype("malgunbd.ttf", 40)

def process_frame(frame, holistic):
    img = cv2.flip(frame, 1)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    result = holistic.process(img_rgb)
    return img, result

if st.session_state.running:
    # ì›¹ìº  ì—´ê¸°
    cap = cv2.VideoCapture(0)
    cap.set(cv2.CAP_PROP_FPS, 20) 
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    
    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:
        # ë¬´í•œ ë£¨í”„ ëŒ€ì‹  ë°˜ë³µë¬¸ì„ ëŒë©° í”„ë ˆì„ ì²˜ë¦¬
        # Streamlit ì•±ì€ ìƒí˜¸ì‘ìš© ì‹œ ë§¤ë²ˆ rerunë˜ë¯€ë¡œ whileë¬¸ì„ ì œí•œì ìœ¼ë¡œ ì‚¬ìš©
        # ì—¬ê¸°ëŠ” ë¬´í•œ ë£¨í”„ ì˜ˆì‹œì§€ë§Œ, ì‹¤ì œ ë°°í¬ ì‹œì—ëŠ” break ì¡°ê±´ì„ ì¶”ê°€í•˜ê±°ë‚˜ streamlit-webrtc í™œìš© ê¶Œì¥
        while True:
            ret, frame = cap.read()
            if not ret:
                st.warning("ì¹´ë©”ë¼ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                break
            
            img, result = process_frame(frame, holistic)
            
            seq = []
            action_seq = []
            lang = []
            font = ImageFont.truetype("malgunbd.ttf", 40)

            # Mediapipe ê²°ê³¼ ê·¸ë¦¬ê¸°
            mp_drawing.draw_landmarks(img, result.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)
            mp_drawing.draw_landmarks(img, result.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)
            mp_drawing.draw_landmarks(img, result.pose_landmarks, mp_holistic.POSE_CONNECTIONS)
            
            # ê¸°ì¡´ ë¡œì§ (joint angle ê³„ì‚°, ëª¨ë¸ ì˜ˆì¸¡)
            # ... (ì—¬ê¸°ì„œëŠ” ì „ì²´ ë¡œì§ ê·¸ëŒ€ë¡œ ì´ë™)
            # ì£¼ì˜: ê¸¸ì´ ê´€ê³„ë¡œ ì™„ì „í•œ ë³µë¶™ì€ ì–´ë ¤ìš°ë©° í•µì‹¬ ë¶€ë¶„ë§Œ ë°œì·Œ
            
            # pose, hand landmark ì²˜ë¦¬ ë¡œì§ ìƒëµ(ê¸°ì¡´ ì½”ë“œ ë™ì¼)
            # ì´ ë¶€ë¶„ì—ì„œ st.session_state.seq, st.session_state.action_seq, st.session_state.lang ì—…ë°ì´íŠ¸
            
            # ì´ë¯¸ì§€ í‘œì‹œ
            pil_img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
            draw = ImageDraw.Draw(pil_img)
            
            if st.session_state.lang:
                current_text = " ".join(st.session_state.lang)
            else:
                current_text = "ì¸ì‹ëœ ë‹¨ì–´ ì—†ìŒ"

            draw.text((10, 30), f'{current_text}', font=font, fill=(255, 0, 0))
            img = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)
            
            frame_placeholder.image(img, channels="BGR")
            
            # Streamlitì´ ì¸í„°ë™ì…˜ ì²˜ë¦¬ í›„ ë£¨í”„ë¥¼ ë‹¤ì‹œ ëŒê¸° ìœ„í•´ í•„ìš”
            if not st.session_state.running:
                break
            
            # generate_buttonê³¼ delete_button ìƒíƒœë¥¼ ì²´í¬í•˜ë ¤ë©´ ë§¤ loopë§ˆë‹¤ ì²´í¬
            if generate_button:
                # ë¬¸ì¥ ìƒì„± ë° ì½ê¸° ë¡œì§
                filtered_words = [w for w in st.session_state.lang if w.strip()]
                if filtered_words:
                    result_text = chain.run(filtered_words=" ".join(filtered_words))
                    output_text = result_text.split(': ')[-1].strip().strip("'")
                    convert_text_to_speech(output_text, TYPECAST_API_KEY, TYPECAST_CHARACTER_ID)
                    speak(output_text)
                st.session_state.lang = []

            if delete_button:
                if st.session_state.lang:
                    st.session_state.lang.pop()

            # ì§§ì€ sleepì„ í†µí•´ UI ì—…ë°ì´íŠ¸
            time.sleep(0.05)
    
    cap.release()

else:
    st.info("ì›¹ìº ì´ êº¼ì ¸ ìˆìŠµë‹ˆë‹¤. ì›¹ìº ì„ ì‹œì‘í•˜ë ¤ë©´ ì™¼ìª½ ì‚¬ì´ë“œë°”ì˜ 'ì›¹ìº  ì‹œì‘/ì¤‘ì§€' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")
