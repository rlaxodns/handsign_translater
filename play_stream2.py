import cv2
import mediapipe as mp
import numpy as np
import torch
import requests
import time
import io
import streamlit as st
import sounddevice as sd
from scipy.io.wavfile import read
from PIL import ImageFont, ImageDraw, Image
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.chat_models import ChatOpenAI
from langchain_community.chat_models import ChatOpenAI
from streamlit_webrtc import webrtc_streamer, WebRtcMode, RTCConfiguration

from functions.model import TimeSeriesTransformer

# ===== TTS í•¨ìˆ˜ =====
def convert_text_to_speech(text, api_key, character_id):
    HEADERS = {'Authorization': f'Bearer {api_key}'}
    try:
        r = requests.post('https://typecast.ai/api/speak', headers=HEADERS, json={
            'text': text,
            'lang': 'auto',
            'actor_id': character_id,
            'xapi_hd': True,
            'model_version': 'latest'
        })
        speak_url = r.json()['result']['speak_v2_url']

        for _ in range(60):
            r = requests.get(speak_url, headers=HEADERS)
            ret = r.json()['result']
            if ret['status'] == 'done':
                audio_data = requests.get(ret['audio_download_url']).content
                audio_stream = io.BytesIO(audio_data)
                sample_rate, audio = read(audio_stream)
                sd.play(audio, samplerate=sample_rate)
                sd.wait()
                break
    except Exception as e:
        print(f"TTS conversion error: {e}")

def speak(text, api_key, character_id):
    HEADERS = {'Authorization': f'Bearer {api_key}'}
    r = requests.post('https://typecast.ai/api/speak', headers=HEADERS, json={
        'text': text,
        'lang': 'auto',
        'actor_id': character_id,
        'xapi_hd': True,
        'model_version': 'latest'
    })
    speak_url = r.json()['result']['speak_v2_url']

    for _ in range(60):
        r = requests.get(speak_url, headers=HEADERS)
        ret = r.json()['result']
        if ret['status'] == 'done':
            audio_data = requests.get(ret['audio_download_url']).content
            audio_stream = io.BytesIO(audio_data)
            sample_rate, audio = read(audio_stream)
            sd.play(audio, samplerate=sample_rate)
            sd.wait()
            break

# ===== ì„¤ì • =====
TYPECAST_API_KEY = ""
TYPECAST_CHARACTER_ID = "622964d6255364be41659078"
openai_api_key = 'sk-...'  # ì‹¤ì œí‚¤ë¡œ ë³€ê²½
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

# LLM ì„¤ì •
llm = ChatOpenAI(
    model_name="gpt-3.5-turbo",
    temperature=0.1,
    api_key=openai_api_key,
)

prompt = PromptTemplate(
    input_variables=["filtered_words"],
    template=(
        "ë¦¬ìŠ¤íŠ¸ì— í¬í•¨ë˜ì§€ ì•Šì€ ë‹¨ì–´ë¥¼ ìƒˆë¡­ê²Œ ìƒì„±í•˜ê±°ë‚˜ ë¬¸ì¥ì„ ìƒˆë¡­ê²Œ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”."
        "ì•„ë˜ì˜ ë‹¨ì–´ë“¤ë§Œ ì‚¬ìš©í•´ì„œ ë¬¸ì–´ì²´ ë¬¸ì¥ì„ ì‘ì„±í•˜ì„¸ìš”. "
        "ë°˜ë“œì‹œ ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” ë‹¨ì–´ë§Œ ì‚¬ìš©í•˜ê³ , í•„ìš” ì—†ëŠ” ë‹¨ì–´ëŠ” ì‚­ì œí•˜ì„¸ìš”. "
        "ì¶œë ¥ì€ ë‹¤ìŒ ì¡°ê±´ì„ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤:\n"
        "1. ë¬¸ì¥ì€ í•œ ë¬¸ì¥ë§Œ ì‘ì„±í•˜ì„¸ìš”.\n"
        "2. ì¶œë ¥ ë¬¸ì¥ì€ ë§ˆì¹¨í‘œ(.)ë¡œ ëë‚˜ì•¼ í•©ë‹ˆë‹¤.\n"
        "3. êµ¬ì–´ì²´ë‚˜ ë¶€ì ì ˆí•œ í‘œí˜„ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.\n"
        "4. ë‹¨ì–´ì˜ ë¬¸ë§¥ì  ì˜ë¯¸ë¥¼ ê³ ë ¤í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ ì‚¬ìš©í•˜ì„¸ìš”.\n"
        "5. ë¦¬ìŠ¤íŠ¸ ë‚´ì˜ ë‹¨ì–´ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ì„ ì¶œë ¥í•  ìˆ˜ ì—†ë‹¤ë©´, '-'ë¡œ ì¶œë ¥í•˜ì„¸ìš”\n"
        "ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸: {filtered_words}\n"
    )
)

chain = LLMChain(llm=llm, prompt=prompt)

# ëª¨ë¸ ë¡œë“œ
model = TimeSeriesTransformer(224, 64, 8, 1, 37, 30, 1)
model_weights_path = r'C:\Users\kim\Desktop\ëŒ€ë‚˜ë¬´íŒ€\custom-final\models\model_final_custom4.pt'
model = torch.load(model_weights_path, map_location=device)
model.eval()

gesture = {0: "", 1: "ì•ˆë…•", 2: "ë§Œë‚˜ë‹¤", 3: "ë°˜ê°‘ë‹¤", 4: "ë‚˜", 5: "ì…ë‹ˆë‹¤", 6: "ì´ë¦„", 7: "ì§€í˜œ", 8: "ê¸°í˜„", 9: "íƒœìš´",
           10: "ì¢‹ì•„í•´", 11: "ì´ˆë°¥", 12: "íŒŒìŠ¤íƒ€", 13: "ì±…", 14: "ìš”ë¦¬", 15: "ê³ ë§ˆì›Œ", 16: "ë‹¤ìŒ", 17: "ë‹¤ì‹œ", 18: "ì†Œê°œ", 19: "ìš”ì¦˜",
           20: "ìˆ˜í™”", 21: "ë°°ìš°ë‹¤", 22: "ì»´í“¨í„°", 23: "ì—´ì‹¬íˆ", 24: "ë…¸ë ¥", 25:"ì¤‘ì´ë‹¤", 26: "ë¬´ì–¼", 27: "ìƒê°", 28: "ì–´ì œ", 29: "ë¨¹ë‹¤",
           30: "ë§›ìˆë‹¤", 31: "ì´ìœ ", 32: "ì½ë‹¤", 33: "ë“£ë‹¤", 34: "ë„ˆ", 35: "ì¹˜í‚¨", 36: "ëŒ€í•™ì›"}

actions = ["", "ì•ˆë…•", "ë§Œë‚˜ë‹¤", "ë°˜ê°‘ë‹¤", "ë‚˜", "ì…ë‹ˆë‹¤", "ì´ë¦„", "ì§€í˜œ", "ê¸°í˜„", "íƒœìš´",
           "ì¢‹ì•„í•´", "ì´ˆë°¥", "íŒŒìŠ¤íƒ€", "ì±…", "ìš”ë¦¬", "ê³ ë§ˆì›Œ", "ë‹¤ìŒ", "ë‹¤ì‹œ", "ì†Œê°œ", "ìš”ì¦˜",
           "ìˆ˜í™”", "ë°°ìš°ë‹¤", "ì»´í“¨í„°", "ì—´ì‹¬íˆ", "ë…¸ë ¥", "ì¤‘ì´ë‹¤", "ë¬´ì–¼", "ìƒê°", "ì–´ì œ", "ë¨¹ë‹¤",
           "ë§›ìˆë‹¤", "ì´ìœ ", "ì½ë‹¤", "ë“£ë‹¤", "ë„ˆ", "ì¹˜í‚¨", "ëŒ€í•™ì›"]

seq_length = 30

mp_holistic = mp.solutions.holistic
mp_drawing = mp.solutions.drawing_utils
font = ImageFont.truetype("malgunbd.ttf", 40)

# ì„¸ì…˜ ìƒíƒœ ê´€ë¦¬
if "seq" not in st.session_state:
    st.session_state.seq = []
if "action_seq" not in st.session_state:
    st.session_state.action_seq = []
if "lang" not in st.session_state:
    st.session_state.lang = []
if "last_added_time" not in st.session_state:
    st.session_state.last_added_time = 0


st.set_page_config(page_title="ìˆ˜ì–´ ì¸ì‹ Demo", layout="wide")
st.title("ğŸ“¹ ì‹¤ì‹œê°„ ìˆ˜ì–´ ì¸ì‹ Demo")

# WebRTC ì„¤ì •
RTC_CONFIGURATION = RTCConfiguration(
    {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
)

# ë¹„ë””ì˜¤ ì²˜ë¦¬ìš© í´ë˜ìŠ¤
class VideoTransformer:
    def __init__(self):
        self.holistic = mp_holistic.Holistic(
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )

    def calc_hand_angle(self, hand_landmarks):
        if hand_landmarks is None:
            return np.zeros((99,))
        joint = np.zeros((21,4))
        for j, lm in enumerate(hand_landmarks.landmark):
            joint[j] = [lm.x, lm.y, lm.z, lm.visibility]

        v1 = joint[[0,1,2,3,0,5,6,7,0,9,10,11,0,13,14,15,0,17,18,19],:]
        v2 = joint[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],:]
        v = v2 - v1
        # 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²½ìš° íšŒí”¼
        norm = np.linalg.norm(v, axis=1)
        norm[norm==0] = 1e-6
        v = v / norm[:, np.newaxis]

        angle = np.arccos(
            np.einsum(
                "nt,nt->n",
                v[[0,1,2,4,5,6,8,9,10,12,13,14,16,17,18],:],
                v[[1,2,3,5,6,7,9,10,11,13,14,15,17,18,19],:]
            )
        )
        angle = np.degrees(angle)
        angle = np.concatenate([joint.flatten(), angle])
        return angle

    def calc_pose_angle(self, pose_landmarks):
        if pose_landmarks is None:
            return np.zeros((27,))
        joint = np.zeros((6,4))
        idxs = [11,12,13,14,23,24]
        k = 0
        for j, lm in enumerate(pose_landmarks.landmark):
            if j in idxs:
                joint[k] = [lm.x, lm.y, lm.z, lm.visibility]
                k+=1

        v1 = joint[[0,1,1,0,0,4],:]
        v2 = joint[[1,3,5,2,4,5],:]
        norm = np.linalg.norm(v2 - v1, axis=1)
        norm[norm==0] = 1e-6
        v = (v2 - v1) / norm[:, np.newaxis]
        angle = np.arccos(np.einsum("nt,nt->n", v[[0,1],:], v[[2,3],:]))
        angle = np.degrees(angle)
        angle = np.concatenate([joint.flatten(), angle])
        return angle

    def transform(self, frame):
        # frame: av.VideoFrame
        img = frame.to_ndarray(format="bgr24")

        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        result = self.holistic.process(img_rgb)

        # drawing landmarks
        mp_drawing.draw_landmarks(img, result.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)
        mp_drawing.draw_landmarks(img, result.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)
        mp_drawing.draw_landmarks(img, result.pose_landmarks, mp_holistic.POSE_CONNECTIONS)

        # íŠ¹ì§• ì¶”ì¶œ
        right = self.calc_hand_angle(result.right_hand_landmarks)
        left = self.calc_hand_angle(result.left_hand_landmarks)

        pose = self.calc_pose_angle(result.pose_landmarks)
        data = np.concatenate([right, left, pose])
        st.session_state.seq.append(data)

        if len(st.session_state.seq) >= seq_length:
            input_data = np.array(st.session_state.seq[-seq_length:], dtype=np.float32)
            input_data = torch.tensor(input_data, dtype=torch.float32, device=device).unsqueeze(0)
            y_pred = model(input_data)
            conf, idx = torch.max(y_pred.data, dim=1)
            if conf > 0.25:
                action = actions[idx.item()]
                st.session_state.action_seq.append(action)
                if len(st.session_state.action_seq) >= 10:
                    if st.session_state.action_seq[-5:] == [action]*5:
                        if time.time() - st.session_state.last_added_time > 2:
                            if action and (not st.session_state.lang or st.session_state.lang[-1] != action):
                                st.session_state.lang.append(action)
                                st.session_state.last_added_time = time.time()

        # ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ í™”ë©´ì— í‘œì‹œ
        pil_img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
        draw = ImageDraw.Draw(pil_img)
        current_text = " ".join(st.session_state.lang) if st.session_state.lang else "ì¸ì‹ëœ ë‹¨ì–´ ì—†ìŒ"
        draw.text((10, 30), f'{current_text}', font=font, fill=(255, 0, 0))
        img = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)

        return img

# WebRTC ìŠ¤íŠ¸ë¦¬ë¨¸
webrtc_ctx = webrtc_streamer(
    key="example",
    mode=WebRtcMode.SENDRECV,
    rtc_configuration=RTC_CONFIGURATION,
    media_stream_constraints={"video": True, "audio": False},
    video_transformer_factory=VideoTransformer,
    async_transform=True
)

col1, col2 = st.columns(2)

with col2:
    if st.button("ë¬¸ì¥ ìƒì„± ë° ì½ê¸°"):
        filtered_words = [w for w in st.session_state.lang if w.strip()]
        if filtered_words:
            result_text = chain.run(filtered_words=" ".join(filtered_words))
            st.write("ìƒì„± ë¬¸ì¥:", result_text)
            output_text = result_text.split(': ')[-1].strip().strip("'")
            convert_text_to_speech(output_text, TYPECAST_API_KEY, TYPECAST_CHARACTER_ID)
            speak(output_text, TYPECAST_API_KEY, TYPECAST_CHARACTER_ID)
        else:
            st.write("ìƒì„±í•  ë¬¸ì¥ì´ ì—†ìŠµë‹ˆë‹¤.")
        st.session_state.lang = []

    if st.button("ë§ˆì§€ë§‰ ë‹¨ì–´ ì‚­ì œ"):
        if st.session_state.lang:
            st.session_state.lang.pop()
            st.write("ì‚­ì œ í›„ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸:", st.session_state.lang)
